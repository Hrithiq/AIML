{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hrith\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os \n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import PIL\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "with open(\"intents.json\") as file:\n",
    "    data=json.load(file)\n",
    "\n",
    "words=[]\n",
    "labels=[]\n",
    "doc_y=[]\n",
    "doc_x=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hrith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        pattern=pattern.lower()\n",
    "\n",
    "        wrds=nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        doc_x.append(wrds)\n",
    "        doc_y.append(intent['tag'])\n",
    "\n",
    "        if intent['tag'] not in labels:\n",
    "            labels.append(intent['tag'])\n",
    "\n",
    "stemmer=LancasterStemmer()\n",
    "words=[stemmer.stem(w.lower()) for w in words if w not in \"?\"]\n",
    "words=sorted(list(set(words)))\n",
    "labels=sorted(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_empty = [0 for _ in range(len(labels))]\n",
    "training=[]\n",
    "output=[]\n",
    "for x, doc in enumerate(doc_x):\n",
    "    bag = []\n",
    "    wrds = [stemmer.stem(w) for w in doc]\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(doc_y[x])] = 1\n",
    "    training.append(bag)\n",
    "    output.append(output_row)    \n",
    "    \n",
    "    #Converting our training data to numpy arrays\n",
    "training = np.array(training)\n",
    "output = np.array(output)\n",
    "\n",
    "#Saving our data\n",
    "with open(\"data.pickle\", \"wb\") as f:\n",
    "    pickle.dump((words, labels, training, output), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7999  | total loss: \u001b[1m\u001b[32m0.12420\u001b[0m\u001b[0m | time: 0.014s\n",
      "| Adam | epoch: 1000 | loss: 0.12420 - acc: 0.9889 -- iter: 56/59\n",
      "Training Step: 8000  | total loss: \u001b[1m\u001b[32m0.11650\u001b[0m\u001b[0m | time: 0.016s\n",
      "| Adam | epoch: 1000 | loss: 0.11650 - acc: 0.9900 -- iter: 59/59\n",
      "--\n",
      "INFO:tensorflow:c:\\Users\\hrith\\OneDrive\\Desktop\\Project Manas\\GDSC_Workshop\\Chatbot_WS\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "net= tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)   \n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")    \n",
    "net = tflearn.regression(net)\n",
    "model= tflearn.DNN(net)\n",
    "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
